{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b09a642-3b2f-4b83-a9e2-c909e0860cb7",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align: center;\">\n",
    "    Assignment 2: Image Classification\n",
    "</h1>\n",
    "<div style=\"text-align: right;\"> <b>\n",
    "    Name: Pronoy Patra <br>\n",
    "    Roll No.: 2021112019\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41a655d-b848-44b6-9dba-c965a1393b38",
   "metadata": {},
   "source": [
    "## Data preparation and rules\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "433048ff-636e-4a53-b74c-df2b73f0947d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training images: 60000\n",
      "Number of test images: 10000\n",
      "Shape of X_train: (60000, 784)\n",
      "Shape of y_train: (60000,)\n",
      "Shape of X_test: (10000, 784)\n",
      "Shape of y_test: (10000,)\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# Transformations \n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(), # Convert images to tensors\n",
    "    transforms.Normalize((0.5,), (0.5,)) # Normalize the images\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "subset_indices = range(6000)\n",
    "train_dataset_6k = Subset(train_dataset, subset_indices)\n",
    "\n",
    "num_train_images = len(train_dataset)\n",
    "num_test_images = len(test_dataset)\n",
    "\n",
    "print(f\"Number of training images: {num_train_images}\")\n",
    "print(f\"Number of test images: {num_test_images}\")\n",
    "\n",
    "# Extracting X and y from the dataset\n",
    "X_train = []\n",
    "y_train = []\n",
    "for image, label in train_dataset:\n",
    "    X_train.append(image.numpy().flatten())\n",
    "    y_train.append(label)\n",
    "\n",
    "X_test = []\n",
    "y_test = []\n",
    "for image, label in test_dataset:\n",
    "    X_test.append(image.numpy().flatten())\n",
    "    y_test.append(label)\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "print(\"Shape of X_train:\", X_train.shape)\n",
    "print(\"Shape of y_train:\", y_train.shape)\n",
    "print(\"Shape of X_test:\", X_test.shape)\n",
    "print(\"Shape of y_test:\", y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97cbe80-80f9-4a4b-9739-492fc95cb0ca",
   "metadata": {},
   "source": [
    "## Q1: SIFT-BoVW-SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d86d19-ed40-4405-aecf-ecddbcd8b5eb",
   "metadata": {},
   "source": [
    "### 1. Implement the SIFT detector and descriptor. Compute cluster centers for the Bag-of-Visual-Words approach. Represent the images as histograms (of visual words) and train a linear SVM model for 10-way classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3218b35-6fd6-4253-8fb4-cb9380b32452",
   "metadata": {},
   "source": [
    "In Bag of Words (BOW) approach:\n",
    "We use image features as visual words. Key point location will be SIFT Features described by SIFT Detectors. \n",
    "\n",
    "Then, we count how often do such visual words appear in an image. Which is basically creating an histogram of it. \n",
    "\n",
    "We group several features (using clustering algorithm) together to one word, Mean feature descriptor whihch are computed from similar descriptors at the visual word. \n",
    "\n",
    "The visual words that results from the clustering are called the Dictinary. \n",
    "\n",
    "All images are now reduced to histograms. All comparisions are performed only using the histograms, not the images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c4188644-1ce8-4483-874b-b7cf1d31eba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7076\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import load_digits\n",
    "\n",
    "# Seed random numbers for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Step 1: Feature Extraction with SIFT\n",
    "def extract_sift_features(image):\n",
    "    sift = cv2.SIFT_create()\n",
    "    keypoints, descriptors = sift.detectAndCompute(image, None)\n",
    "    return keypoints, descriptors\n",
    "\n",
    "# Step 2: Clustering to form Visual Words\n",
    "def build_visual_vocabulary(descriptors, k):\n",
    "    kmeans = KMeans(n_clusters=k)\n",
    "    kmeans.fit(descriptors)\n",
    "    return kmeans.cluster_centers_\n",
    "\n",
    "# Step 3: Encoding Images as Histograms\n",
    "def encode_image_histogram(image, visual_vocabulary):\n",
    "    keypoints, descriptors = extract_sift_features(image)\n",
    "    histogram = np.zeros(len(visual_vocabulary))\n",
    "    if descriptors is not None:\n",
    "        for descriptor in descriptors:\n",
    "            distances = np.linalg.norm(visual_vocabulary - descriptor, axis=1)\n",
    "            closest_visual_word_index = np.argmin(distances)\n",
    "            histogram[closest_visual_word_index] += 1\n",
    "    return histogram\n",
    "\n",
    "# Step 4: Training Linear SVM Model\n",
    "def train_svm_model(X_train, y_train):\n",
    "    svm = make_pipeline(StandardScaler(), SVC(kernel='linear'))\n",
    "    svm.fit(X_train, y_train)\n",
    "    return svm\n",
    "\n",
    "# Extract SIFT descriptors from images\n",
    "descriptors = []\n",
    "for image in X_train:\n",
    "    _, des = extract_sift_features(image.reshape((28, 28)).astype(np.uint8))\n",
    "    if des is not None:\n",
    "        descriptors.extend(des)\n",
    "\n",
    "# Building visual vocabulary\n",
    "k = 50  # Number of visual words\n",
    "visual_vocabulary = build_visual_vocabulary(np.array(descriptors), k)\n",
    "\n",
    "# Encode train images as histograms\n",
    "X_train_histograms = []\n",
    "for image in X_train:\n",
    "    histogram = encode_image_histogram(image.reshape((28, 28)).astype(np.uint8), visual_vocabulary)\n",
    "    X_train_histograms.append(histogram)\n",
    "\n",
    "# Train SVM model\n",
    "svm_model = train_svm_model(X_train_histograms, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "X_test_histograms = []\n",
    "for image in X_test:\n",
    "    histogram = encode_image_histogram(image.reshape((28, 28)).astype(np.uint8), visual_vocabulary)\n",
    "    X_test_histograms.append(histogram)\n",
    "\n",
    "y_pred = svm_model.predict(X_test_histograms)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a5af6b-3a0c-4bcb-84ae-a197ac2849ea",
   "metadata": {},
   "source": [
    "If a word occur in every image then it will not help in comparisions. So, we can use wights depending on the imformation it is going to convey. \n",
    "This is done using TF-IDF: Term Frequency Inverse Document Frequency. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79bd7b6c-c4f3-4835-8b30-47c5c9be667c",
   "metadata": {},
   "source": [
    "$t_{id} = \\frac{f_{nid}}{n_{d}}\\log\\frac{N}{n_{i}}$\n",
    "\n",
    "$t_{id}:$ histogram bin of word $i$ for image $d$\n",
    "\n",
    "$n_{id}:$ occurences of word $i$ in image $d$\n",
    "\n",
    "$n_d:$ number of word occurences in image $d$\n",
    "\n",
    "$n_i:$ number of images that contain word $i$\n",
    "\n",
    "$N:$ number of images\n",
    "\n",
    "This method can increase its accuracy even more by focusing on the more important visual words than all the words. \r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8683a98b-179e-4a89-a368-9a314da65755",
   "metadata": {},
   "source": [
    "### 2. Keeping everything else constant, plot how classification accuracy changes as you sweep across 6 different values for the number of clusters. Please decide what numbers are meaningful for this question. Explain the trends in classification accuracy that you observe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "62f98127-71e0-47a3-968b-5bd84e2cd807",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Cluster = 1\tAccuracy: 0.2153\n",
      "Number of Cluster = 3\tAccuracy: 0.2795\n",
      "Number of Cluster = 10\tAccuracy: 0.4588\n",
      "Number of Cluster = 100\tAccuracy: 0.7744\n",
      "Number of Cluster = 300\tAccuracy: 0.8352\n",
      "Number of Cluster = 600\tAccuracy: 0.8513\n",
      "[0.2153, 0.2795, 0.4588, 0.7744, 0.8352, 0.8513]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_cluster = [1,3,10,100,300,600]\n",
    "accuracy_cluster = []\n",
    "\n",
    "for k in num_cluster:\n",
    "    # Build visual vocabulary\n",
    "    # k: Number of visual words\n",
    "    print(f\"Number of Cluster = {k}\", end = '\\t')\n",
    "    visual_vocabulary = build_visual_vocabulary(np.array(descriptors), k)\n",
    "    \n",
    "    # Encode train images as histograms\n",
    "    X_train_histograms = []\n",
    "    for image in X_train:\n",
    "        histogram = encode_image_histogram(image.reshape((28, 28)).astype(np.uint8), visual_vocabulary)\n",
    "        X_train_histograms.append(histogram)\n",
    "    \n",
    "    # Train SVM model\n",
    "    svm_model = train_svm_model(X_train_histograms, y_train)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    X_test_histograms = []\n",
    "    for image in X_test:\n",
    "        histogram = encode_image_histogram(image.reshape((28, 28)).astype(np.uint8), visual_vocabulary)\n",
    "        X_test_histograms.append(histogram)\n",
    "    \n",
    "    y_pred = svm_model.predict(X_test_histograms)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    accuracy_cluster.append(accuracy)\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "\n",
    "print(accuracy_cluster)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(num_cluster, accuracy_cluster)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Variation in accuracy with number of clusters')\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c789301c-da9b-4329-90a4-38cc928f2639",
   "metadata": {},
   "source": [
    "**What numbers are meaningful for this question**\n",
    "\n",
    "Only those numbers would be crucial with whom we can detect SIFT features. Otherwise, if we cannot detect any SIFT or visual features then it cannot be used for training and hence are of no use.\n",
    "\n",
    "**Trend in the classification accuracy**\n",
    "\n",
    "We can observe that as the number of cluster increases the accuracy increases as well. This is because more visual words are considered in BoVW. Resulting in to consideration of more visual features and hence better accuracy. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf9e110-36e9-462e-948e-e40555a0a54d",
   "metadata": {},
   "source": [
    "### 3. Show the results for 6 different hyperparameter settings. You may play with the SIFT detector or descriptor and the linear SVM. Keep the number of clusters constant based on the answer to the previous question. Explain the trends in classification accuracy that you observe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243f1999-27ed-4b81-85a7-0784fbff257b",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_list = [1, 0.5, 1.5]\n",
    "loss_list = ['squared_hinge', 'hinge']\n",
    "k = 50\n",
    "accuracies = []\n",
    "for c in c_list:\n",
    "    for loss_function in loss_list:\n",
    "        print(f\"C = {c}\\tLoss Function = {loss_function}\", end = '\\t')\n",
    "        k = 50  # Number of visual words\n",
    "        visual_vocabulary = build_visual_vocabulary(np.array(descriptors), k)\n",
    "        \n",
    "        # Encode train images as histograms\n",
    "        X_train_histograms = []\n",
    "        for image in X_train:\n",
    "            histogram = encode_image_histogram(image.reshape((28, 28)).astype(np.uint8), visual_vocabulary)\n",
    "            X_train_histograms.append(histogram)\n",
    "        \n",
    "        # Train SVM model\n",
    "        \n",
    "        svm = make_pipeline(StandardScaler(), svm.LinearSVC(dual = 'auto', C = c, loss = loss_function))\n",
    "        svm.fit(X_train_histograms, y_train)\n",
    "        \n",
    "        # Evaluate the model\n",
    "        X_test_histograms = []\n",
    "        for image in X_test:\n",
    "            histogram = encode_image_histogram(image.reshape((28, 28)).astype(np.uint8), visual_vocabulary)\n",
    "            X_test_histograms.append(histogram)\n",
    "        \n",
    "        y_pred = svm_model.predict(X_test_histograms)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac739437-c1e4-44cd-a35a-0d85e0c9246f",
   "metadata": {},
   "source": [
    "**Trends in Classification Accuracy**\n",
    "\n",
    "**C**\n",
    "\n",
    "The regularization parameter (C) in SVMs controls the trade-off between maximizing the margin and minimizing the classification error. \n",
    "\n",
    "<u>High Regularization (Small C):</u>\n",
    "\n",
    "When C is small, the SVM maximizes the margin and allows for a larger number of misclassifications.\n",
    "\n",
    "Small C values lead to a simpler decision boundary, often with a larger margin between classes resulting in robust and generalized model, less prone to overfitting.\n",
    "\n",
    "However, too much regularization (very small C values) may lead to underfitting, where the model is too simplistic and fails to capture the complexities of the data.\n",
    "\n",
    "<u>Low Regularization (Large C):</u>\n",
    "\n",
    "When C is large, the SVM minimizes the classification error at the expense of a smaller margin.\n",
    "\n",
    "Large C values allow the SVM to be more flexible in fitting the training data, potentially leading to a more complex decision boundary.\n",
    "\n",
    "A larger C value can make the SVM more sensitive to outliers and noisy data points, potentially leading to overfitting.\n",
    "\n",
    "**Loss Function**\n",
    "\n",
    "<u>Hinge Loss:</u>\n",
    "\n",
    "It penalizes misclassifications by the amount by which they violate the margin.\n",
    "\n",
    "Hinge loss encourages maximizing the margin between classes, which is desirable for digit recognition where clear class boundaries are essential.\n",
    "\n",
    "The hinge loss is convex and leads to convex optimization problems, making it computationally efficient to solve.\n",
    "\n",
    "<u>Squared Hinge Loss:</u>\n",
    "\n",
    "It is a variant of the hinge loss that penalizes misclassifications more severely. It squares the margin violations, leading to a smoother optimization landscape.\n",
    "\n",
    "Squared hinge loss can be more robust to outliers compared to hinge loss, as it penalizes large margin violations more strongly.\n",
    "ameter C. test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af669408-e9a1-429b-82c3-d194554ca931",
   "metadata": {},
   "source": [
    "## Q2: CNNs and Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c399a606-fcf8-4078-8ea6-77b9123a33fc",
   "metadata": {},
   "source": [
    "### 1. Set up a modular codebase for training a CNN (LeNet) on the task of handwritten digit recognition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f2fba5-19b0-4f85-815c-b5e3c1fd5f10",
   "metadata": {},
   "source": [
    "#### Implement logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b562896c-df37-4db1-8fc9-ba6a10dd3536",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpronoy-patra\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\BPATR\\OneDrive\\Desktop\\cv\\a2\\wandb\\run-20240308_181104-8sc86eiw</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/pronoy-patra/logging_CNN/runs/8sc86eiw' target=\"_blank\">hyperparameters</a></strong> to <a href='https://wandb.ai/pronoy-patra/logging_CNN' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/pronoy-patra/logging_CNN' target=\"_blank\">https://wandb.ai/pronoy-patra/logging_CNN</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/pronoy-patra/logging_CNN/runs/8sc86eiw' target=\"_blank\">https://wandb.ai/pronoy-patra/logging_CNN/runs/8sc86eiw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "project_name = 'logging_CNN'\n",
    "group_name = 'CNN'\n",
    "experiment_name = 'hyperparameters'\n",
    "wandb.init(\n",
    "    project=project_name,\n",
    "    group=group_name,\n",
    "    name=experiment_name,\n",
    "    config={\n",
    "        \"optimizer\": \"Adam\",\n",
    "        \"loss\": \"crossentropy\",  \n",
    "        \"metric\": \"accuracy\",\n",
    "        \"epoch\": 10\n",
    "    })\n",
    "config = wandb.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "53c4cb7e-7e2c-4d35-a6e0-9e1178a2ec04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment: Batch Size=64, Learning Rate=0.001, Optimizer=Adam\n",
      "Epoch 1/10, Training Loss: 0.2617\n",
      "Epoch 2/10, Training Loss: 0.0652\n",
      "Epoch 3/10, Training Loss: 0.0450\n",
      "Epoch 4/10, Training Loss: 0.0350\n",
      "Epoch 5/10, Training Loss: 0.0300\n",
      "Epoch 6/10, Training Loss: 0.0251\n",
      "Epoch 7/10, Training Loss: 0.0224\n",
      "Epoch 8/10, Training Loss: 0.0181\n",
      "Epoch 9/10, Training Loss: 0.0179\n",
      "Epoch 10/10, Training Loss: 0.0143\n",
      "Test Accuracy: 0.9955\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import MNIST\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# LeNet model\n",
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, kernel_size=5)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(256, 120)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(self.relu1(self.conv1(x)))\n",
    "        x = self.pool2(self.relu2(self.conv2(x)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.relu3(self.fc1(x))\n",
    "        x = self.relu4(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Training the model\n",
    "def train_model(train_loader, model, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "# Evaluating the model\n",
    "def evaluate_model(data_loader, model, device):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in data_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predictions = torch.max(outputs, 1)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_predictions.extend(predictions.cpu().numpy())\n",
    "            \n",
    "    accuracy = accuracy_score(all_labels, all_predictions)\n",
    "    return accuracy\n",
    "\n",
    "# Clear functional separation between the data (dataset and dataloader)\n",
    "def get_data_loaders(train_dataset_para, batch_size=64):\n",
    "    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "    train_loader = DataLoader(dataset=train_dataset_para, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "    test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "    return train_loader, test_loader\n",
    "\n",
    "# Training and evaluation for different hyperparameter settings - Wandb loggin\n",
    "def run_experiment_wandb(batch_size, learning_rate, optimizer_name, num_epochs, dataset_para, device):\n",
    "    train_loader, test_loader = get_data_loaders(dataset_para, batch_size=batch_size)\n",
    "\n",
    "    model = LeNet().to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    if optimizer_name == 'SGD':\n",
    "        optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    elif optimizer_name == 'Adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported optimizer\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = train_model(train_loader, model, criterion, optimizer, device)\n",
    "        evaluation_loss = train_model(test_loader, model, criterion, optimizer, device)\n",
    "\n",
    "        train_accuracy = evaluate_model(train_loader, model, device)\n",
    "        evaluation_accuracy = evaluate_model(test_loader, model, device)\n",
    "        \n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}, Training Loss: {train_loss:.4f}')\n",
    "        \n",
    "        wandb.log({\"Train loss\":train_loss})\n",
    "        wandb.log({\"Evaluation accuracy\":train_accuracy})\n",
    "\n",
    "        wandb.log({\"Evaluation loss\":evaluation_loss})\n",
    "        wandb.log({\"Evaluation accuracy\":evaluation_accuracy})\n",
    "\n",
    "    accuracy = evaluate_model(test_loader, model, device)\n",
    "    print(f'Test Accuracy: {accuracy:.4f}\\n')\n",
    "\n",
    "# No wandb login - Used for hyperparameter tuning\n",
    "def run_experiment(batch_size, learning_rate, optimizer_name, num_epochs, dataset_para, device):\n",
    "    train_loader, test_loader = get_data_loaders(dataset_para, batch_size=batch_size)\n",
    "\n",
    "    model = LeNet().to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    if optimizer_name == 'SGD':\n",
    "        optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    elif optimizer_name == 'Adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported optimizer\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = train_model(train_loader, model, criterion, optimizer, device)\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}, Training Loss: {train_loss:.4f}')\n",
    "\n",
    "    accuracy = evaluate_model(test_loader, model, device)\n",
    "    print(f'Test Accuracy: {accuracy:.4f}\\n')\n",
    "\n",
    "batch_size=64\n",
    "learning_rate=0.001\n",
    "optimizer_name='Adam'\n",
    "num_epochs = 10\n",
    "print(f\"Experiment: Batch Size={batch_size}, Learning Rate={learning_rate}, Optimizer={optimizer_name}\")\n",
    "run_experiment_wandb(batch_size, learning_rate, optimizer_name, num_epochs, train_dataset, device='cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d446dc5e-53e2-42a0-b8ff-e35c9795be44",
   "metadata": {},
   "source": [
    "### 2. Show the results for 6 different settings of hyperparameters. You may want to change the batch size, learning rate, and optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "351f1a52-975f-4167-95d0-e6e169dfff17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment: Batch Size=64, Learning Rate=0.01, Optimizer=SGD\n",
      "Epoch 1/5, Training Loss: 1.8765\n",
      "Epoch 2/5, Training Loss: 0.3234\n",
      "Epoch 3/5, Training Loss: 0.1678\n",
      "Epoch 4/5, Training Loss: 0.1193\n",
      "Epoch 5/5, Training Loss: 0.0951\n",
      "Test Accuracy: 0.9750\n",
      "\n",
      "Experiment: Batch Size=128, Learning Rate=0.001, Optimizer=SGD\n",
      "Epoch 1/5, Training Loss: 2.3045\n",
      "Epoch 2/5, Training Loss: 2.3014\n",
      "Epoch 3/5, Training Loss: 2.2985\n",
      "Epoch 4/5, Training Loss: 2.2954\n",
      "Epoch 5/5, Training Loss: 2.2920\n",
      "Test Accuracy: 0.1441\n",
      "\n",
      "Experiment: Batch Size=64, Learning Rate=0.001, Optimizer=Adam\n",
      "Epoch 1/5, Training Loss: 0.2590\n",
      "Epoch 2/5, Training Loss: 0.0665\n",
      "Epoch 3/5, Training Loss: 0.0492\n",
      "Epoch 4/5, Training Loss: 0.0391\n",
      "Epoch 5/5, Training Loss: 0.0328\n",
      "Test Accuracy: 0.9868\n",
      "\n",
      "Experiment: Batch Size=128, Learning Rate=0.01, Optimizer=Adam\n",
      "Epoch 1/5, Training Loss: 0.1884\n",
      "Epoch 2/5, Training Loss: 0.0757\n",
      "Epoch 3/5, Training Loss: 0.0658\n",
      "Epoch 4/5, Training Loss: 0.0593\n",
      "Epoch 5/5, Training Loss: 0.0551\n",
      "Test Accuracy: 0.9851\n",
      "\n",
      "Experiment: Batch Size=32, Learning Rate=0.001, Optimizer=SGD\n",
      "Epoch 1/5, Training Loss: 2.3012\n",
      "Epoch 2/5, Training Loss: 2.2881\n",
      "Epoch 3/5, Training Loss: 2.2595\n",
      "Epoch 4/5, Training Loss: 2.1255\n",
      "Epoch 5/5, Training Loss: 1.2108\n",
      "Test Accuracy: 0.8419\n",
      "\n",
      "Experiment: Batch Size=32, Learning Rate=0.01, Optimizer=Adam\n",
      "Epoch 1/5, Training Loss: 0.2051\n",
      "Epoch 2/5, Training Loss: 0.1384\n",
      "Epoch 3/5, Training Loss: 0.1207\n",
      "Epoch 4/5, Training Loss: 0.1247\n",
      "Epoch 5/5, Training Loss: 0.1303\n",
      "Test Accuracy: 0.9704\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Set hyperparameters for different experiments\n",
    "# Changing batch size, learning rate, and optimizer\n",
    "\n",
    "hyperparameter_settings = [\n",
    "    (64, 0.01, 'SGD', 5),\n",
    "    (128, 0.001, 'SGD', 5),\n",
    "    (64, 0.001, 'Adam', 5),\n",
    "    (128, 0.01, 'Adam', 5),\n",
    "    (32, 0.001, 'SGD', 5),\n",
    "    (32, 0.01, 'Adam', 5)\n",
    "]\n",
    "\n",
    "# Run experiments\n",
    "for setting in hyperparameter_settings:\n",
    "    batch_size, learning_rate, optimizer_name, num_epochs = setting\n",
    "    print(f\"Experiment: Batch Size={batch_size}, Learning Rate={learning_rate}, Optimizer={optimizer_name}\")\n",
    "    run_experiment(batch_size, learning_rate, optimizer_name, num_epochs, train_dataset, device='cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e7d5d9-eb9d-445f-8b27-ec9435d2f5fd",
   "metadata": {},
   "source": [
    "**Trends in classification accuracy**\n",
    "\n",
    "As we can observe from the values that:\n",
    "* <u>Learning Rate:</u> Low learning rate gives better accuracies, but too less learning rates can lead to slower convergence.\n",
    "* <u>Optimizer:</u> We observe that Adam optimizer is better than SGD in all the cases, becauses it utlises the concepts of adaptive learning rate and momentum.\n",
    "* <u>Batch Size:</u> We observe that the more the batch size, the model reaches convergence earlier as they lead to training stability, i.e., smoother gradient estimation by averaging gradients over more samples.\n",
    "\n",
    "**Which hyperparameters are most important?**\n",
    "\n",
    "Apart from Learning Rate, batch size, number of epochs and optimizers, there are multiple other hyperparameters that are crucial while training a CNN. \n",
    "* <u>Network architecture:</u> This includes number of layers, the size of the layers, the type of layers (e.g., convolutional, pooling, fully connected), and the connectivity between layers. \n",
    "* <u>Activation Function:</u> The choice of activation function can affect the model's training speed and performance. Like ReLU (Rectified Linear Unit), tanh, and sigmoid. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e399a9f-1580-4ffb-974b-3b95f8495670",
   "metadata": {},
   "source": [
    "### 3. Compare the best performing CNN (from above) against the SIFT-BoVW-SVM approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f93ef64-fe22-42f8-ae68-a3af43c5b5f9",
   "metadata": {},
   "source": [
    "As we can observe from above that CNN (Accuracy more than 95%) are much better than SIFT-BoVW-SVM approach (Accuracy around 70%).\n",
    "Major reasons behind this are: \n",
    "\n",
    "* <u>Feature Learning:</u>\n",
    "\n",
    "**CNNs:** CNNs extract hierarchical features from images during training leading to better performance.\n",
    "\n",
    "**SIFT-BoVW-SVM:** It relies on SIFT and BoVW for feature representation and might not capture all the relevant information in complex datasets, limiting the approach's performance.\n",
    "\n",
    "* <u>Performance:</u>\n",
    "\n",
    "**CNNs:** They can learn complex patterns and hierarchical representations directly from data, leading to high accuracy.\n",
    "\n",
    "**SIFT-BoVW-SVM:** The performance is limited and can give bad accuracies for datasets containing significant variability or large-scale variations.\n",
    "tions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0703bb9f-0f1f-439e-ac62-22bf721aaebe",
   "metadata": {},
   "source": [
    "### 4. How does the performance change if you double the number of convolutional layers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "316eda24-c7e9-4dd2-8238-801e9fca21ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 98 %\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define the LeNet model\n",
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, kernel_size=5, padding=2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)\n",
    "        self.conv3 = nn.Conv2d(16, 120, kernel_size=5)\n",
    "        self.conv4 = nn.Conv2d(120, 84, kernel_size=1)\n",
    "        self.fc1 = nn.Linear(84 * 1 * 1, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.max_pool2d(x, kernel_size=2, stride=2)\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = torch.max_pool2d(x, kernel_size=2, stride=2)\n",
    "        x = torch.relu(self.conv3(x))\n",
    "        x = torch.relu(self.conv4(x))\n",
    "        x = x.view(-1, 84 * 1 * 1)\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Load MNIST dataset\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = LeNet()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()          \n",
    "\n",
    "# Testing\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        images, labels = data\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "    100 * correct / total))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130f54bb-24f5-4939-b013-5166775f7766",
   "metadata": {},
   "source": [
    "Adding more convolutional layers increases the capacity of the network to extract hierarchical features from the input data improving the model's ability to learn complex patterns and representations, leading to better performance. \n",
    "\n",
    "But the number of parameters have increased leading to more number of parameters and hence more time and space complexity. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ffe15a-9acc-406d-806b-d062c52e5404",
   "metadata": {},
   "source": [
    "### 5. How does the performance change as you increase the number of training samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1f940f15-fdce-4eef-8607-be4623b7f481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment: Batch Size=64, Learning Rate=0.001, Optimizer=Adam\n",
      "Training Samples: 600\n",
      "Epoch 1/5, Training Loss: 2.2903\n",
      "Epoch 2/5, Training Loss: 2.1951\n",
      "Epoch 3/5, Training Loss: 1.9328\n",
      "Epoch 4/5, Training Loss: 1.4190\n",
      "Epoch 5/5, Training Loss: 0.9353\n",
      "Test Accuracy: 0.7468\n",
      "\n",
      "\n",
      "\n",
      "Training Samples: 1800\n",
      "Epoch 1/5, Training Loss: 2.0885\n",
      "Epoch 2/5, Training Loss: 1.0325\n",
      "Epoch 3/5, Training Loss: 0.5556\n",
      "Epoch 4/5, Training Loss: 0.3758\n",
      "Epoch 5/5, Training Loss: 0.3100\n",
      "Test Accuracy: 0.9128\n",
      "\n",
      "\n",
      "\n",
      "Training Samples: 6000\n",
      "Epoch 1/5, Training Loss: 1.1428\n",
      "Epoch 2/5, Training Loss: 0.3049\n",
      "Epoch 3/5, Training Loss: 0.1936\n",
      "Epoch 4/5, Training Loss: 0.1434\n",
      "Epoch 5/5, Training Loss: 0.1120\n",
      "Test Accuracy: 0.9611\n",
      "\n",
      "\n",
      "\n",
      "Training Samples: 18000\n",
      "Epoch 1/5, Training Loss: 0.6072\n",
      "Epoch 2/5, Training Loss: 0.1578\n",
      "Epoch 3/5, Training Loss: 0.1030\n",
      "Epoch 4/5, Training Loss: 0.0820\n",
      "Epoch 5/5, Training Loss: 0.0649\n",
      "Test Accuracy: 0.9778\n",
      "\n",
      "\n",
      "\n",
      "Training Samples: 60000\n",
      "Epoch 1/5, Training Loss: 0.2640\n",
      "Epoch 2/5, Training Loss: 0.0730\n",
      "Epoch 3/5, Training Loss: 0.0535\n",
      "Epoch 4/5, Training Loss: 0.0428\n",
      "Epoch 5/5, Training Loss: 0.0368\n",
      "Test Accuracy: 0.9888\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import MNIST\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# LeNet model\n",
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, kernel_size=5)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(256, 120)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(self.relu1(self.conv1(x)))\n",
    "        x = self.pool2(self.relu2(self.conv2(x)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.relu3(self.fc1(x))\n",
    "        x = self.relu4(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Training the model\n",
    "def train_model(train_loader, model, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "# Evaluating the model\n",
    "def evaluate_model(data_loader, model, device):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in data_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predictions = torch.max(outputs, 1)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_predictions.extend(predictions.cpu().numpy())\n",
    "            \n",
    "    accuracy = accuracy_score(all_labels, all_predictions)\n",
    "    return accuracy\n",
    "\n",
    "# Clear functional separation between the data (dataset and dataloader)\n",
    "def get_data_loaders(train_dataset_para, batch_size=64):\n",
    "    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "    train_loader = DataLoader(dataset=train_dataset_para, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "    test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "    return train_loader, test_loader\n",
    "\n",
    "def run_experiment(batch_size, learning_rate, optimizer_name, num_epochs, dataset_para, device):\n",
    "    train_loader, test_loader = get_data_loaders(dataset_para, batch_size=batch_size)\n",
    "\n",
    "    model = LeNet().to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    if optimizer_name == 'SGD':\n",
    "        optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    elif optimizer_name == 'Adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported optimizer\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = train_model(train_loader, model, criterion, optimizer, device)\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}, Training Loss: {train_loss:.4f}')\n",
    "\n",
    "    accuracy = evaluate_model(test_loader, model, device)\n",
    "    print(f'Test Accuracy: {accuracy:.4f}\\n')\n",
    "\n",
    "\n",
    "batch_size=64\n",
    "learning_rate=0.001\n",
    "optimizer_name='Adam'\n",
    "num_epochs = 5\n",
    "print(f\"Experiment: Batch Size={batch_size}, Learning Rate={learning_rate}, Optimizer={optimizer_name}\")\n",
    "\n",
    "samples =  [600, 1800, 6000, 18000, 60000]\n",
    "for s in samples: \n",
    "    # Select a subset of 6000 images\n",
    "    subset_indices = range(s)\n",
    "    train_dataset_samples = Subset(train_dataset, subset_indices)\n",
    "    print(f\"Training Samples: {s}\")\n",
    "    run_experiment(batch_size, learning_rate, optimizer_name, num_epochs, train_dataset_samples, device='cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print('\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb4b5c1-70f3-4a6d-b096-8894735675b0",
   "metadata": {},
   "source": [
    "**Explain the trends in classification accuracy**\n",
    "\n",
    "We can observe that as the training samples increases, the accuracy of the model increases. THis is because as the sample size increases the CNN tends to learn better and hence can learn the features more effectively. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324e732a-9434-4f12-9a5f-0cec3f3f8f91",
   "metadata": {},
   "source": [
    "### 6. 2 layer TransformerEncoder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d710ee4-7a71-4e86-8bb0-536b20daebe1",
   "metadata": {},
   "source": [
    "#### ViT style prediction scheme, evaluate classification accuracy when training with 60K - Positional Encoding: Sinusoids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fb53b4aa-2ebe-46ec-bedc-f557c8db21e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BPATR\\OneDrive\\Desktop\\cv\\cvenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 1.0374\n",
      "Epoch [2/5], Loss: 0.2592\n",
      "Epoch [3/5], Loss: 0.1745\n",
      "Epoch [4/5], Loss: 0.1372\n",
      "Epoch [5/5], Loss: 0.1214\n",
      "Accuracy on test set: 0.9688\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import accuracy_score\n",
    "import math\n",
    "\n",
    "# Define positional encoding\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "# Define the model\n",
    "class ViT(nn.Module):\n",
    "    def __init__(self, patch_size=7, hidden_dim=64, num_classes=10, num_layers=2, num_heads=4, dropout=0.1):\n",
    "        super(ViT, self).__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_classes = num_classes\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # Linear projection for patches\n",
    "        self.linear_projection = nn.Linear(patch_size * patch_size, hidden_dim)\n",
    "\n",
    "        # Positional encoding\n",
    "        self.positional_encoding = PositionalEncoding(hidden_dim, dropout)\n",
    "\n",
    "        # Transformer Encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=hidden_dim, nhead=num_heads)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        # Classifier head\n",
    "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Convert the 28x28 images to 7x7 patches\n",
    "        x = x.unfold(2, self.patch_size, self.patch_size).unfold(3, self.patch_size, self.patch_size)\n",
    "        x = x.contiguous().view(x.size(0), x.size(2) * x.size(3), -1)\n",
    "        \n",
    "        # Linear projection for patches\n",
    "        x = self.linear_projection(x)\n",
    "        \n",
    "        # Add positional encoding\n",
    "        x = self.positional_encoding(x)\n",
    "\n",
    "        # Transformer Encoder\n",
    "        x = self.transformer_encoder(x)\n",
    "\n",
    "        # Classifier head\n",
    "        x = x.mean(1)  # Global average pooling\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# MNIST dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "test_dataset = torchvision.datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n",
    "\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = ViT().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * images.size(0)\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {train_loss:.4f}\")\n",
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "predictions = []\n",
    "true_labels = []\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        predictions.extend(predicted.cpu().numpy())\n",
    "        true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "accuracy = accuracy_score(true_labels, predictions)\n",
    "print(f\"Accuracy on test set: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41bfe80-c1f3-45c8-b84f-dcddabf53ff5",
   "metadata": {},
   "source": [
    "#### ViT style prediction scheme, evaluate classification accuracy when training with 6K - Postional Encoding: Sinusoids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b1fd70da-5440-4f27-b4a7-0a41f5070b9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BPATR\\OneDrive\\Desktop\\cv\\cvenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 2.2322\n",
      "Epoch [2/5], Loss: 1.7926\n",
      "Epoch [3/5], Loss: 1.3977\n",
      "Epoch [4/5], Loss: 1.0848\n",
      "Epoch [5/5], Loss: 0.8184\n",
      "Accuracy on test set: 0.8087\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import accuracy_score\n",
    "import math\n",
    "\n",
    "# Define positional encoding\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "# Define the model\n",
    "class ViT(nn.Module):\n",
    "    def __init__(self, patch_size=7, hidden_dim=64, num_classes=10, num_layers=2, num_heads=4, dropout=0.1):\n",
    "        super(ViT, self).__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_classes = num_classes\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # Linear projection for patches\n",
    "        self.linear_projection = nn.Linear(patch_size * patch_size, hidden_dim)\n",
    "\n",
    "        # Positional encoding\n",
    "        self.positional_encoding = PositionalEncoding(hidden_dim, dropout)\n",
    "\n",
    "        # Transformer Encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=hidden_dim, nhead=num_heads)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        # Classifier head\n",
    "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Convert the 28x28 images to 7x7 patches\n",
    "        x = x.unfold(2, self.patch_size, self.patch_size).unfold(3, self.patch_size, self.patch_size)\n",
    "        x = x.contiguous().view(x.size(0), x.size(2) * x.size(3), -1)\n",
    "        \n",
    "        # Linear projection for patches\n",
    "        x = self.linear_projection(x)\n",
    "        \n",
    "        # Add positional encoding\n",
    "        x = self.positional_encoding(x)\n",
    "\n",
    "        # Transformer Encoder\n",
    "        x = self.transformer_encoder(x)\n",
    "\n",
    "        # Classifier head\n",
    "        x = x.mean(1)  # Global average pooling\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# MNIST dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "test_dataset = torchvision.datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n",
    "\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(dataset=train_dataset_6k, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = ViT().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * images.size(0)\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {train_loss:.4f}\")\n",
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "predictions = []\n",
    "true_labels = []\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        predictions.extend(predicted.cpu().numpy())\n",
    "        true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "accuracy = accuracy_score(true_labels, predictions)\n",
    "print(f\"Accuracy on test set: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f05b70-1624-4640-8338-cc6c7ae96ec7",
   "metadata": {},
   "source": [
    "#### ViT style prediction scheme, evaluate classification accuracy when training with 60K - Postional Encoding: Random Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c618e95e-5389-456d-a51f-3fef64b08377",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BPATR\\OneDrive\\Desktop\\cv\\cvenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 0.8282\n",
      "Epoch [2/5], Loss: 0.1983\n",
      "Epoch [3/5], Loss: 0.1369\n",
      "Epoch [4/5], Loss: 0.1119\n",
      "Epoch [5/5], Loss: 0.0965\n",
      "Accuracy on test set: 0.9681\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# Define the model\n",
    "class ViT(nn.Module):\n",
    "    def __init__(self, patch_size=7, hidden_dim=64, num_classes=10, num_layers=2, num_heads=4, dropout=0.1):\n",
    "        super(ViT, self).__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_classes = num_classes\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # Linear projection for patches\n",
    "        self.linear_projection = nn.Linear(patch_size * patch_size, hidden_dim)\n",
    "\n",
    "        # Random positional encoding\n",
    "        self.positional_encoding = nn.Parameter(torch.randn(1, hidden_dim))\n",
    "\n",
    "        # Transformer Encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=hidden_dim, nhead=num_heads)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        # Classifier head\n",
    "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Convert the 28x28 images to 7x7 patches\n",
    "        x = x.unfold(2, self.patch_size, self.patch_size).unfold(3, self.patch_size, self.patch_size)\n",
    "        x = x.contiguous().view(x.size(0), x.size(2) * x.size(3), -1)\n",
    "        \n",
    "        # Linear projection for patches\n",
    "        x = self.linear_projection(x)\n",
    "        \n",
    "        # Add positional encoding\n",
    "        x = x + self.positional_encoding\n",
    "\n",
    "        # Transformer Encoder\n",
    "        x = self.transformer_encoder(x)\n",
    "\n",
    "        # Classifier head\n",
    "        x = x.mean(1)  # Global average pooling\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# MNIST dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# train_dataset = torchvision.datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "# test_dataset = torchvision.datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n",
    "\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = ViT().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * images.size(0)\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {train_loss:.4f}\")\n",
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "predictions = []\n",
    "true_labels = []\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        predictions.extend(predicted.cpu().numpy())\n",
    "        true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "accuracy = accuracy_score(true_labels, predictions)\n",
    "print(f\"Accuracy on test set: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8994ca1-5967-41a6-917b-2765213132c6",
   "metadata": {},
   "source": [
    "#### ViT style prediction scheme, evaluate classification accuracy when training with 6K - Postional Encoding: Random Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8000d3a1-235b-4b3b-a26e-60ea13b0898a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BPATR\\OneDrive\\Desktop\\cv\\cvenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 2.1691\n",
      "Epoch [2/5], Loss: 1.6331\n",
      "Epoch [3/5], Loss: 1.1398\n",
      "Epoch [4/5], Loss: 0.7950\n",
      "Epoch [5/5], Loss: 0.5723\n",
      "Accuracy on test set: 0.8810\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# Define the model\n",
    "class ViT(nn.Module):\n",
    "    def __init__(self, patch_size=7, hidden_dim=64, num_classes=10, num_layers=2, num_heads=4, dropout=0.1):\n",
    "        super(ViT, self).__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_classes = num_classes\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # Linear projection for patches\n",
    "        self.linear_projection = nn.Linear(patch_size * patch_size, hidden_dim)\n",
    "\n",
    "        # Random positional encoding\n",
    "        self.positional_encoding = nn.Parameter(torch.randn(1, hidden_dim))\n",
    "\n",
    "        # Transformer Encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=hidden_dim, nhead=num_heads)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        # Classifier head\n",
    "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Convert the 28x28 images to 7x7 patches\n",
    "        x = x.unfold(2, self.patch_size, self.patch_size).unfold(3, self.patch_size, self.patch_size)\n",
    "        x = x.contiguous().view(x.size(0), x.size(2) * x.size(3), -1)\n",
    "        \n",
    "        # Linear projection for patches\n",
    "        x = self.linear_projection(x)\n",
    "        \n",
    "        # Add positional encoding\n",
    "        x = x + self.positional_encoding\n",
    "\n",
    "        # Transformer Encoder\n",
    "        x = self.transformer_encoder(x)\n",
    "\n",
    "        # Classifier head\n",
    "        x = x.mean(1)  # Global average pooling\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# MNIST dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# train_dataset = torchvision.datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "# test_dataset = torchvision.datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n",
    "\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(dataset=train_dataset_6k, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = ViT().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * images.size(0)\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {train_loss:.4f}\")\n",
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "predictions = []\n",
    "true_labels = []\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        predictions.extend(predicted.cpu().numpy())\n",
    "        true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "accuracy = accuracy_score(true_labels, predictions)\n",
    "print(f\"Accuracy on test set: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a261e289-d85a-4633-83ba-b62cd7c18f22",
   "metadata": {},
   "source": [
    "We can observe that training with 60k dataset always outperforms 6k dataset. As more data leads to better training of the model and hence better accuracies. \n",
    "\n",
    "**How do the results compare against CNNs?**\n",
    "\n",
    "In CNN we observe lesser accuracy compared to ViT.?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e9b581-05ed-4c37-a950-57e24023114d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
